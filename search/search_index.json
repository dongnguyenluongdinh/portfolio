{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to my documentation page!","text":"<p>Here, I share my technical projects and experiences to collaborate with other enthusiasts seeking practical guides and innovative solutions.</p>"},{"location":"#introduction","title":"Introduction","text":""},{"location":"#projects","title":"Projects","text":"<ul> <li>Building a YOLO-Powered Chatbot</li> <li>Python Environment Setup with Bash</li> </ul>"},{"location":"#notes","title":"Notes","text":"<ul> <li>Next Steps and Future Projects</li> </ul>"},{"location":"blog_gpu_setup/","title":"TensorFlow GPU Setup in WSL2","text":"<p>Native GPU Support on Windows is Deprecated: TensorFlow 2.10 is the last version to support GPU natively on Windows.</p> <p>From 2.11 onward:</p> <ul> <li> <p>Native GPU support was removed</p> </li> <li> <p>CUDA-based GPU usage only works on Linux or WSL2</p> </li> </ul>"},{"location":"blog_gpu_setup/#installation","title":"Installation","text":"<p>This guide focuses on using <code>TensorFlow 2.10</code> in combination with <code>CUDA 11.8</code> for optimal GPU acceleration. For further information on compatibility between TensorFlow and CUDA versions, you can refer to NVIDIA Optimized Frameworks - TensorFlow Release Notes.</p>"},{"location":"blog_gpu_setup/#install-wsl2-ubuntu","title":"\u2705 Install WSL2 + Ubuntu","text":"<p><pre><code>wsl --install -d Ubuntu\n</code></pre> For a successful installation, output should be like this:</p> <pre><code>&gt;&gt;&gt; wsl --install -d Ubuntu\nInstalling: Ubuntu\nUbuntu has been successfully installed.\nPress any key to continue...\n</code></pre>"},{"location":"blog_gpu_setup/#set-up-nvidia-gpu","title":"\u2705 Set Up NVIDIA GPU","text":"<p>Prerequisites:</p> <ul> <li> <p>Windows 10 19044 or higher (64-bit).</p> </li> <li> <p>NVIDIA\u00ae GPU card with CUDA\u00ae architectures 3.5, 5.0, 6.0, 7.0, 7.5, 8.0 and higher: CUDA\u00ae-enabled GPU cards</p> </li> <li> <p>NVIDIA GPU drivers 525.105 or newer</p> </li> </ul> <p><pre><code>&gt;&gt;&gt; nvidia-smi\n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 537.13                 Driver Version: 537.13       CUDA Version: 11.8     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA GeForce RTX 3060 ...  WDDM  | 00000000:01:00.0 Off |                  N/A |\n| N/A   39C    P8               9W /  95W |      0MiB /  6144MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n</code></pre> - WSL2 CUDA support packages installed</p> <p>Add CUDA 11.8 APT Repo for WSL2:</p> <pre><code>wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin\nsudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600\n\nwget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda-repo-wsl-ubuntu-11-8-local_11.8.0-1_amd64.deb\nsudo dpkg -i cuda-repo-wsl-ubuntu-11-8-local_11.8.0-1_amd64.deb\nsudo cp /var/cuda-repo-wsl-ubuntu-11-8-local/cuda-*-keyring.gpg /usr/share/keyrings/\nsudo apt-get update\n</code></pre> <p>Install CUDA Toolkit 11.8 <pre><code>sudo apt install -y cuda-toolkit-11-8\n</code></pre></p> <p>Update <code>.bashrc</code>:</p> <p><pre><code>export PATH=/usr/local/cuda-11.8/bin:$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH\n</code></pre> Refresh <code>.bashrc</code> and apply changes to the shell:</p> <p><pre><code>source ~/.bashrc\n</code></pre> Test CUDA 11.8 Installation</p> <pre><code>git clone https://github.com/NVIDIA/cuda-samples.git \ncd cuda-samples/Samples/1_Utilities/deviceQuery\ncmake . # (1)!\nmake \n./deviceQuery \n</code></pre> <ol> <li>Before running <code>make</code>, you need to generate the Makefiles using <code>CMake</code>. If <code>Cmake</code> not available, install with:     <pre><code>sudo apt install cmake\n</code></pre></li> </ol> <p>Then run the built <code>deviceQuery</code> with:</p> <pre><code>&gt;&gt;&gt; ./deciveQuery\n./deviceQuery Starting...\n\n CUDA Device Query (Runtime API) version (CUDART static linking)\n\nDetected 1 CUDA Capable device(s)\n\nDevice 0: \"NVIDIA GeForce RTX 3060 Laptop GPU\"\n  CUDA Driver Version / Runtime Version          12.2 / 11.8\n  CUDA Capability Major/Minor version number:    8.6\n  Total amount of global memory:                 6144 MBytes (6441926656 bytes)\n  (030) Multiprocessors, (128) CUDA Cores/MP:    3840 CUDA Cores\n  GPU Max Clock rate:                            1425 MHz (1.42 GHz)\n  Memory Clock rate:                             7001 Mhz\n  Memory Bus Width:                              192-bit\n  L2 Cache Size:                                 3145728 bytes\n  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n  Total amount of constant memory:               65536 bytes\n  Total amount of shared memory per block:       49152 bytes\n  Total shared memory per multiprocessor:        102400 bytes\n  Total number of registers available per block: 65536\n  Warp size:                                     32\n  Maximum number of threads per multiprocessor:  1536\n  Maximum number of threads per block:           1024\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n  Maximum memory pitch:                          2147483647 bytes\n  Texture alignment:                             512 bytes\n  Concurrent copy and kernel execution:          Yes with 1 copy engine(s)\n  Run time limit on kernels:                     Yes\n  Integrated GPU sharing Host Memory:            No\n  Support host page-locked memory mapping:       Yes\n  Alignment requirement for Surfaces:            Yes\n  Device has ECC support:                        Disabled\n  Device supports Unified Addressing (UVA):      Yes\n  Device supports Managed Memory:                Yes\n  Device supports Compute Preemption:            Yes\n  Supports Cooperative Kernel Launch:            Yes\n  Supports MultiDevice Co-op Kernel Launch:      No\n  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\n  Compute Mode:\n     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;\n\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.2, CUDA Runtime Version = 11.8, NumDevs = 1\nResult = PASS\n</code></pre>"},{"location":"blog_gpu_setup/#install-tensorflow","title":"\u2705 Install TensorFlow","text":"<pre><code>uv venv\nsource .venv/bin/activate\nuv pip install tensorflow # (1)!\n</code></pre> <ol> <li>The latest tensorflow from PyPI on Linux includes GPU support out of the box, but for stability select <code>tensorflow 2.10</code>.</li> </ol>"},{"location":"blog_gpu_setup/#test-gpu-access","title":"\u2705 Test GPU Access","text":"<p>Create a <code>gpu_check.py</code> like this:</p> <pre><code>import tensorflow as tf\n\nprint(\"TensorFlow Version:\", tf.__version__)\nprint(\"GPUs:\", tf.config.list_physical_devices('GPU'))\n</code></pre> <p>Then execute it using:</p> <pre><code>&gt;&gt;&gt; python gpu_check.py\nTensorFlow Version: 2.10.0\nGPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n</code></pre>"},{"location":"blog_gpu_setup/#error-handling","title":"Error handling","text":"<p>To be continue ...</p>"},{"location":"introduction/","title":"Introduction","text":"<p>Most of my projects revolve around the versatility and power of Python, making it my go-to  programming language for tackling a wide range of challenges. </p> <p>Whether it's automating workflows, building intelligent chatbots, or crafting efficient scripts,  Python stands at the core of my work.</p>"},{"location":"introduction/#installation","title":"Installation","text":""},{"location":"introduction/#virtual-environment","title":"Virtual Environment","text":"<p>Create a virtual environment with:</p> venvuv <pre><code>python -m venv .venv\n</code></pre> <pre><code>uv venv .venv # (1)!\n</code></pre> <ol> <li> <p><code>uv venv .venv</code> ensures that all virtual environments are created with the name <code>.venv</code>. To use the current directory name as the virtual environment's name, run:</p> <pre><code>uv venv\n</code></pre> </li> </ol> <p>Activate the virtual environment with:</p> PowerShellBash <pre><code>.\\.venv\\Scripts\\activate\n</code></pre> <pre><code>source .venv/Script/activate\n</code></pre> <p>Install dependencies with:</p> uvpip <pre><code>uv pip install -r requirements.txt\n</code></pre> <pre><code>pip install -r requirements.txt \n</code></pre> <p>When working on Python projects, it\u2019s crucial to ensure that dependencies are  consistent across different environments. A <code>requirements.txt</code> file helps achieve  this by specifying the exact versions of packages used in your project.</p> uvpip <pre><code>uv pip freeze &gt; requirements.txt\n</code></pre> <pre><code>pip freeze &gt; requirements.txt\n</code></pre>"},{"location":"yolo_chatbot/","title":"Construction Defects Detection Chatbot","text":""},{"location":"yolo_chatbot/#overview","title":"Overview","text":"<p>The Construction Defects Detection Chatbot is a <code>YOLO11</code> model designed to identify construction defects from images. The model categorizes defects into three classes:</p> <ul> <li> <p>broken-brick</p> </li> <li> <p>drip-mortar</p> </li> <li> <p>lintel-damage</p> </li> </ul>"},{"location":"yolo_chatbot/#installation","title":"Installation","text":""},{"location":"yolo_chatbot/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Python 3.10+</p> </li> <li> <p>uv installed.</p> </li> </ul>"},{"location":"yolo_chatbot/#steps-to-install","title":"Steps to Install","text":"<p>Create and activate a virtual environment:</p> <pre><code>uv venv\nsource .venv/Scripts/activate\n</code></pre> <p>Install the required dependencies:</p> <pre><code>uv pip install -r requirements.txt\n</code></pre>"},{"location":"yolo_chatbot/#set-up","title":"Set up","text":""},{"location":"yolo_chatbot/#frontend","title":"Frontend","text":"<p>Open <code>index.html</code> with Live Server on port 5500.</p>"},{"location":"yolo_chatbot/#backend","title":"Backend","text":"<p>Run the FastAPI development server with the following command:</p> <pre><code>fastapi dev app.py\n</code></pre>"}]}